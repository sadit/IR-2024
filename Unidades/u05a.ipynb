{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fc189d2-656e-41d4-be84-dcf6ccf9537b",
   "metadata": {},
   "source": [
    "# Modelado semántico\n",
    "Autor: Eric S. Tellez <eric.tellez@infotec.mx>\n",
    "\n",
    "El modelado léxico tiene importantes usos en la recuperación de información; los sistemas pueden ser altamente escalables si hay una implementación adecuada en índice invertido. Sin embargo, sufren de diversos problemas debido a su naturaleza:\n",
    "\n",
    "- Las consultas deben ser expresadas con los mismos términos que los documentos.\n",
    "- Los términos con múltiples significados pueden introducir falsos positivos en los resultados.\n",
    "- En general se tienen problemas con la sinónimia, la homonimia, y la hiponimia, así como las variaciones léxicas resultantes de errores ortográficos, ya sean intencionados o involuntarios.\n",
    "\n",
    "Aunque existen soluciones dos grandes vertientes de modelado semántico:\n",
    "- Modelado del vocabulario: Intententa asignar cada palabra del vocabulario a un _concepto_ o significado _semántico_, que esta representado por un vector de alta dimensión (embedding).\n",
    "- Modelado del lenguaje: Intenta modelar el lenguaje, normalmente aprendiendo que palabras pueden seguir a un contexto dado. La diferencia principal al modelado de vocabulario es que cuando se modela el lenguaje se tiene un modelo sensible al contexto, y por lo tanto, para una misma palabra, el contexto completo derminará su vector _embedding_. \n",
    "\n",
    "Aunque existen diversos intentos de modelado semántico en la literatura clásica, hasta tiempos recientes es cuando ha tomado gran interés debido a los impresionantes avances en redes neuronales profundas que han permitido aprender estos modelos de enormes colecciones, capturando una semántica profunda. Los grandes modelos de lenguaje han abierto nuevas posibilidades sobre las aplicaciones posibles en recuperación de información.\n",
    "\n",
    "## Sobre las representaciones de documentos\n",
    "Los modelos semánticos de palabras, anteriormente discutidos, son insuficientes para la representación de documentos; para conseguir representaciones de documentos hay diferentes estrategias. Por ejemplo, es posible generar combinaciones lineales de los embeddings individuales para generar vector embedding de documento. Algunos modelos basados en redes neuronales, generan embeddings de documento por diseño.\n",
    "\n",
    "\n",
    "## Organización del resto de la unidad\n",
    "Esta únidad esta dividida en dos notebooks. El primero (este documento) está dedicado al modelado semántico de vocabularios, mientras que el segundo se enfoca a modelos de lenguaje que trabajen sobre documentos. \n",
    "\n",
    "# Modelado semántico de vocabularios\n",
    "El modelado basado en vocabularios o léxico tiene una gran cantidad de aplicaciones, las cuales son suelen explotar la gran cantidad de grados de libertad en el preprocesamiento, toquenizado y pesado, para ajustar a las tareas deseadas. Sin embargo, dicho modelado tiene problemas basados en la semántica:\n",
    "\n",
    "- si las consultas tienen palabras no conocidas (fuera de vocabulario), no es posible ligar a palabras similares por semántica \n",
    "- las palabras similares en el vocabulario pero no iguales, no puede relacionarse para reducir un vocabulario, o para cualquier otro efecto\n",
    "- las palabras que se escriben igual pero que tienen significado idéntico serán puestas en la misma bolsa\n",
    "\n",
    "De manera más detallada, se tendrán problemas al manejar sinónimos, hipónimos, antónimos, etc. Adicionalmente, los errores serán tratados como palabras diferentes y no ajustados al mejor calce conocido (a menos que exista un corrector ortográfico entre el preprocesamiento).\n",
    "\n",
    "Estos problemas se buscan reducir con representaciones semánticas de los vocabularios y de los textos, esto sería equivalente. El intento sería buscar por conceptos más que por palabras que lo describan.\n",
    "En este punto aparecen los modelos semánticos, los cuales suelen ser representaciones vectoriales densas _embeddings_, de alta dimensión pero mucho menor que las representaciones léxicas (i.e., pasarían a unos pocos cientos en lugar de varias decenas de miles o cientos de miles de coordinadas).\n",
    "\n",
    "# Word embeddings\n",
    "Las representaciones de vectores densos de las palabras se suelen construir utilizando la [_hipótesis distribucional_](https://aclweb.org/aclwiki/Distributional_Hypothesis), que se puede resumir que las palabras con contextos similares (palabras que la rodean) tienen un significado similar. Entonces, para obtener la semántica, es necesario comparar contextos; lo anterior se puede llevar a cabo de diferentes maneras.\n",
    "\n",
    "- Latent semantic analysis (LSA): La matriz $W_{m,n}^*$  con pesos basados en frecuencias es usada para reducir el numero de filas (palabras) mediante _Singular Value Decomposition_ (SVD). La similitud adecuada entre vectores en el espacio de dimensión reducida es el coseno. Ver [@Dumais2004] para más información.\n",
    "- Latent semantic indexing (LSI): Similar a LSA pero usando esquemas de pesado diferentes, adicionalmente se usa Trucated SVD (TSVD) en lugar de SVD ya que puede seleccionar componentes más importantes. Para más detalles ver [@Hofman1999].\n",
    "- Word2Vec. Se construye una red neuronal con una matriz de pesos del tamaño del vocabulario por una dimensión seleccionada, a partir de los contextos. Revisa los textos de entrenamiento mediante una ventana móvil. De manera especial, se tiene que el centro de la ventana es la palabra objetivo y el contexto esta a su alrededor. Se intenta predecir mediante el contexto la palabra central, y se ajusta mediante propagación hacia atrás. La función de perdida suele ser [softmax](https://en.wikipedia.org/wiki/Softmax_function) o hierachical softmax. Los embeddings no son la capa de salida sino una capa atrás. Para más detalles ver [@MCCD2013].\n",
    "- Glove. Se utiliza una matriz de coocurrencia de términos dentro de una ventana deslizante que analiza una colección de textos. La matriz es altamente dispersa y de alta dimensión. Plantea un problema de optimización lineal para capturar la información más útil y bajar su dimensión. Los detalles se pueden consultar en [@PSM2014]. \n",
    "- FastText. Muy similar a Word2Vec pero añade manejo de palabras fuera de diccionario mediante subwords (similar a q-gramas de carácteres por cada palabra). Ver [@BGJM2017] para mayores detalles.\n",
    "\n",
    "## Nota\n",
    "Este notebook esta basado en <https://github.com/sadit/SimilaritySearchDemos/blob/main/Glove/Glove.ipynb>. En el notebook original se muestran visualizaciones basadas en todos los vecinos cercanos (se verán más adelante en el curso).\n",
    "\n",
    "# Ejemplo\n",
    "\n",
    "El resto del documento muestra como usar word embeddings Glove para precalculados, para resolver tareas de vecinos cercanos y resolución de analogías."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd04785e-aa4e-493c-9aa8-5c54c8e1f322",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  Activating\u001b[22m\u001b[39m project at `~/Cursos/IR-2024/Unidades`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "using SimilaritySearch, Plots, LinearAlgebra, Embeddings, HypertextLiteral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e406b8-ab57-4157-8c79-cc2d51b30910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af8bad4e-c31b-4506-8341-47c4130d664a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_index (generic function with 1 method)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function load_dataset()\n",
    "    emb = load_embeddings(GloVe, 2)  # you can change with any of the available embeddings in `Embeddings`\n",
    "    for c in eachcol(emb.embeddings)\n",
    "        normalize!(c)\n",
    "    end\n",
    "\n",
    "    db = MatrixDatabase(emb.embeddings)\n",
    "    db, emb.vocab\n",
    "end\n",
    "\n",
    "function create_index()\n",
    "    db, vocab = load_dataset()\n",
    "    dist = NormalizedCosineDistance()\n",
    "    index = SearchGraph(; dist, db, verbose=false)\n",
    "    index!(index)\n",
    "    optimize!(index, MinRecall(0.9))\n",
    "    index, vocab\n",
    "end\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bcb60ff-68c0-4c09-a8dd-70079ba59994",
   "metadata": {},
   "source": [
    "### El índice métrico se crea, además se toma el vocabulario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d9108c0-02a7-4afe-bfa2-7b436f7bd0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 36.649847 seconds (7.40 M allocations: 3.125 GiB, 3.43% gc time, 5.22% compilation time: 3% of which was recompilation)\n"
     ]
    }
   ],
   "source": [
    "@time index, vocab = create_index()\n",
    "vocab2id = Dict(w => i for (i, w) in enumerate(vocab));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e629f24-ecf3-4908-a100-b70559dca89a",
   "metadata": {},
   "source": [
    "### Búsqueda de todos los vecinos cercanos en el vocabulario, observe la conveniencia del uso de un índice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f43463a8-0c73-4903-a78c-6b2405ddf812",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 28.015977 seconds (2.28 M allocations: 5.148 GiB, 0.90% gc time, 2.45% compilation time)\n",
      "360.393454 seconds (230.26 k allocations: 112.083 MiB, 0.07% compilation time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.937028671875"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "let k = 32\n",
    "    @time iI, _ = allknn(index, k)\n",
    "    # WARNING: Don't run the following line, it takes too much time\n",
    "    @time gI, _ = allknn(ExhaustiveSearch(; db=index.db, dist=index.dist), k)\n",
    "    macrorecall(gI, iI)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b68d72-5256-49ad-b22b-e896f58bc29f",
   "metadata": {},
   "source": [
    "### Búsqueda y presentación de los resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e045135e-4189-47cc-9513-72a5000527f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h1>Ejemplos de búsqueda (aleatorios)</h1>"
      ],
      "text/plain": [
       "<h1>Ejemplos de búsqueda (aleatorios)</h1>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000414 seconds (3 allocations: 1.625 KiB)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>resultados for \"oxalic\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>oxalic</td><td><span class=\"IdWeight\">IdWeight(0x00020564, 0.0f0)</span>.id</td><td>0.0</td></tr><tr><td>2</td><td>tartaric</td><td><span class=\"IdWeight\">IdWeight(0x00028f11, 0.23522043f0)</span>.id</td><td>0.235</td></tr><tr><td>3</td><td>benzoic</td><td><span class=\"IdWeight\">IdWeight(0x00024415, 0.24037927f0)</span>.id</td><td>0.24</td></tr><tr><td>4</td><td>picric</td><td><span class=\"IdWeight\">IdWeight(0x00039f8a, 0.25700676f0)</span>.id</td><td>0.257</td></tr><tr><td>5</td><td>salicylic</td><td><span class=\"IdWeight\">IdWeight(0x0001ff29, 0.2570488f0)</span>.id</td><td>0.257</td></tr><tr><td>6</td><td>ferulic</td><td><span class=\"IdWeight\">IdWeight(0x00049f23, 0.27578282f0)</span>.id</td><td>0.276</td></tr><tr><td>7</td><td>hypochlorous</td><td><span class=\"IdWeight\">IdWeight(0x0004704b, 0.27816248f0)</span>.id</td><td>0.278</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<h2>resultados for \"oxalic\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>oxalic</td><td><span class=\"IdWeight\">IdWeight(0x00020564, 0.0f0)</span>.id</td><td>0.0</td></tr><tr><td>2</td><td>tartaric</td><td><span class=\"IdWeight\">IdWeight(0x00028f11, 0.23522043f0)</span>.id</td><td>0.235</td></tr><tr><td>3</td><td>benzoic</td><td><span class=\"IdWeight\">IdWeight(0x00024415, 0.24037927f0)</span>.id</td><td>0.24</td></tr><tr><td>4</td><td>picric</td><td><span class=\"IdWeight\">IdWeight(0x00039f8a, 0.25700676f0)</span>.id</td><td>0.257</td></tr><tr><td>5</td><td>salicylic</td><td><span class=\"IdWeight\">IdWeight(0x0001ff29, 0.2570488f0)</span>.id</td><td>0.257</td></tr><tr><td>6</td><td>ferulic</td><td><span class=\"IdWeight\">IdWeight(0x00049f23, 0.27578282f0)</span>.id</td><td>0.276</td></tr><tr><td>7</td><td>hypochlorous</td><td><span class=\"IdWeight\">IdWeight(0x0004704b, 0.27816248f0)</span>.id</td><td>0.278</td></tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000336 seconds (2 allocations: 32 bytes)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>resultados for \"financieras\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>financieras</td><td><span class=\"IdWeight\">IdWeight(0x0004f268, 5.9604645f-8)</span>.id</td><td>0.0</td></tr><tr><td>2</td><td>culturales</td><td><span class=\"IdWeight\">IdWeight(0x0004a96a, 0.17610365f0)</span>.id</td><td>0.176</td></tr><tr><td>3</td><td>ambientales</td><td><span class=\"IdWeight\">IdWeight(0x0005b339, 0.18060797f0)</span>.id</td><td>0.181</td></tr><tr><td>4</td><td>entidades</td><td><span class=\"IdWeight\">IdWeight(0x0005f250, 0.20086998f0)</span>.id</td><td>0.201</td></tr><tr><td>5</td><td>camaras</td><td><span class=\"IdWeight\">IdWeight(0x000482f0, 0.20159829f0)</span>.id</td><td>0.202</td></tr><tr><td>6</td><td>instituciones</td><td><span class=\"IdWeight\">IdWeight(0x0002311f, 0.20482886f0)</span>.id</td><td>0.205</td></tr><tr><td>7</td><td>franquicias</td><td><span class=\"IdWeight\">IdWeight(0x00056f07, 0.2072379f0)</span>.id</td><td>0.207</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<h2>resultados for \"financieras\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>financieras</td><td><span class=\"IdWeight\">IdWeight(0x0004f268, 5.9604645f-8)</span>.id</td><td>0.0</td></tr><tr><td>2</td><td>culturales</td><td><span class=\"IdWeight\">IdWeight(0x0004a96a, 0.17610365f0)</span>.id</td><td>0.176</td></tr><tr><td>3</td><td>ambientales</td><td><span class=\"IdWeight\">IdWeight(0x0005b339, 0.18060797f0)</span>.id</td><td>0.181</td></tr><tr><td>4</td><td>entidades</td><td><span class=\"IdWeight\">IdWeight(0x0005f250, 0.20086998f0)</span>.id</td><td>0.201</td></tr><tr><td>5</td><td>camaras</td><td><span class=\"IdWeight\">IdWeight(0x000482f0, 0.20159829f0)</span>.id</td><td>0.202</td></tr><tr><td>6</td><td>instituciones</td><td><span class=\"IdWeight\">IdWeight(0x0002311f, 0.20482886f0)</span>.id</td><td>0.205</td></tr><tr><td>7</td><td>franquicias</td><td><span class=\"IdWeight\">IdWeight(0x00056f07, 0.2072379f0)</span>.id</td><td>0.207</td></tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.001120 seconds (3 allocations: 5.906 KiB)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>resultados for \"placente\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>placente</td><td><span class=\"IdWeight\">IdWeight(0x00023cc7, 5.9604645f-8)</span>.id</td><td>0.0</td></tr><tr><td>2</td><td>dorta</td><td><span class=\"IdWeight\">IdWeight(0x0005d8e2, 0.36412215f0)</span>.id</td><td>0.364</td></tr><tr><td>3</td><td>klimowicz</td><td><span class=\"IdWeight\">IdWeight(0x000226f5, 0.36626738f0)</span>.id</td><td>0.366</td></tr><tr><td>4</td><td>cagna</td><td><span class=\"IdWeight\">IdWeight(0x00031e4f, 0.39715785f0)</span>.id</td><td>0.397</td></tr><tr><td>5</td><td>simeone</td><td><span class=\"IdWeight\">IdWeight(0x0000d3e7, 0.4072147f0)</span>.id</td><td>0.407</td></tr><tr><td>6</td><td>chamot</td><td><span class=\"IdWeight\">IdWeight(0x0001fe57, 0.43776006f0)</span>.id</td><td>0.438</td></tr><tr><td>7</td><td>rivarola</td><td><span class=\"IdWeight\">IdWeight(0x000265fd, 0.44890302f0)</span>.id</td><td>0.449</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<h2>resultados for \"placente\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>placente</td><td><span class=\"IdWeight\">IdWeight(0x00023cc7, 5.9604645f-8)</span>.id</td><td>0.0</td></tr><tr><td>2</td><td>dorta</td><td><span class=\"IdWeight\">IdWeight(0x0005d8e2, 0.36412215f0)</span>.id</td><td>0.364</td></tr><tr><td>3</td><td>klimowicz</td><td><span class=\"IdWeight\">IdWeight(0x000226f5, 0.36626738f0)</span>.id</td><td>0.366</td></tr><tr><td>4</td><td>cagna</td><td><span class=\"IdWeight\">IdWeight(0x00031e4f, 0.39715785f0)</span>.id</td><td>0.397</td></tr><tr><td>5</td><td>simeone</td><td><span class=\"IdWeight\">IdWeight(0x0000d3e7, 0.4072147f0)</span>.id</td><td>0.407</td></tr><tr><td>6</td><td>chamot</td><td><span class=\"IdWeight\">IdWeight(0x0001fe57, 0.43776006f0)</span>.id</td><td>0.438</td></tr><tr><td>7</td><td>rivarola</td><td><span class=\"IdWeight\">IdWeight(0x000265fd, 0.44890302f0)</span>.id</td><td>0.449</td></tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function search_and_display(index, vocab, q, res, k, qword)\n",
    "    res = reuse!(res, k)\n",
    "    @time search(index, q, res)\n",
    "\n",
    "    L = []\n",
    "    for (j, p) in enumerate(res)\n",
    "        push!(L, @htl \"<tr><td>$j</td><td>$(vocab[p.id])</td><td>$p.id</td><td>$(round(p.weight, digits=3))</td></tr>\")\n",
    "    end\n",
    "\n",
    "    display(@htl \"\"\"<h2>resultados for \"$qword\"</h2>\n",
    "    <table>\n",
    "    <th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
    "        $L\n",
    "    </table>\n",
    "    \"\"\")\n",
    "end\n",
    "\n",
    "function search_and_display(index, vocab, qid::Integer, res, k=maxlength(res))\n",
    "    search_and_display(index, vocab, index[qid], res, k, vocab[qid])\n",
    "end\n",
    "\n",
    "display(@htl \"<h1>Ejemplos de búsqueda (aleatorios)</h1>\")\n",
    "res = KnnResult(7)\n",
    "for i in 1:3\n",
    "    for qid in rand(1:length(vocab))\n",
    "        search_and_display(index, vocab, qid, res)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b320eacd-d47e-4dbe-a7e9-a240c4535b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000691 seconds (4 allocations: 7.500 KiB)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>resultados for \"glove\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>glove</td><td><span class=\"IdWeight\">IdWeight(0x00002aab, 0.0f0)</span>.id</td><td>0.0</td></tr><tr><td>2</td><td>ball</td><td><span class=\"IdWeight\">IdWeight(0x0000043c, 0.36888665f0)</span>.id</td><td>0.369</td></tr><tr><td>3</td><td>gloves</td><td><span class=\"IdWeight\">IdWeight(0x00002a38, 0.36894822f0)</span>.id</td><td>0.369</td></tr><tr><td>4</td><td>plate</td><td><span class=\"IdWeight\">IdWeight(0x0000110d, 0.3791324f0)</span>.id</td><td>0.379</td></tr><tr><td>5</td><td>pads</td><td><span class=\"IdWeight\">IdWeight(0x00003e6a, 0.4182728f0)</span>.id</td><td>0.418</td></tr><tr><td>6</td><td>infield</td><td><span class=\"IdWeight\">IdWeight(0x0000415b, 0.43464988f0)</span>.id</td><td>0.435</td></tr><tr><td>7</td><td>bounced</td><td><span class=\"IdWeight\">IdWeight(0x000023c2, 0.43528706f0)</span>.id</td><td>0.435</td></tr><tr><td>8</td><td>hat</td><td><span class=\"IdWeight\">IdWeight(0x000015fa, 0.43809587f0)</span>.id</td><td>0.438</td></tr><tr><td>9</td><td>bat</td><td><span class=\"IdWeight\">IdWeight(0x0000133e, 0.43897963f0)</span>.id</td><td>0.439</td></tr><tr><td>10</td><td>fielder</td><td><span class=\"IdWeight\">IdWeight(0x00002801, 0.44676453f0)</span>.id</td><td>0.447</td></tr><tr><td>11</td><td>pitch</td><td><span class=\"IdWeight\">IdWeight(0x00000c1b, 0.45033383f0)</span>.id</td><td>0.45</td></tr><tr><td>12</td><td>helmet</td><td><span class=\"IdWeight\">IdWeight(0x00002b13, 0.4635557f0)</span>.id</td><td>0.464</td></tr><tr><td>13</td><td>catcher</td><td><span class=\"IdWeight\">IdWeight(0x00002369, 0.46529937f0)</span>.id</td><td>0.465</td></tr><tr><td>14</td><td>throw</td><td><span class=\"IdWeight\">IdWeight(0x00000d30, 0.46637392f0)</span>.id</td><td>0.466</td></tr><tr><td>15</td><td>nose</td><td><span class=\"IdWeight\">IdWeight(0x00001737, 0.46784467f0)</span>.id</td><td>0.468</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<h2>resultados for \"glove\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>glove</td><td><span class=\"IdWeight\">IdWeight(0x00002aab, 0.0f0)</span>.id</td><td>0.0</td></tr><tr><td>2</td><td>ball</td><td><span class=\"IdWeight\">IdWeight(0x0000043c, 0.36888665f0)</span>.id</td><td>0.369</td></tr><tr><td>3</td><td>gloves</td><td><span class=\"IdWeight\">IdWeight(0x00002a38, 0.36894822f0)</span>.id</td><td>0.369</td></tr><tr><td>4</td><td>plate</td><td><span class=\"IdWeight\">IdWeight(0x0000110d, 0.3791324f0)</span>.id</td><td>0.379</td></tr><tr><td>5</td><td>pads</td><td><span class=\"IdWeight\">IdWeight(0x00003e6a, 0.4182728f0)</span>.id</td><td>0.418</td></tr><tr><td>6</td><td>infield</td><td><span class=\"IdWeight\">IdWeight(0x0000415b, 0.43464988f0)</span>.id</td><td>0.435</td></tr><tr><td>7</td><td>bounced</td><td><span class=\"IdWeight\">IdWeight(0x000023c2, 0.43528706f0)</span>.id</td><td>0.435</td></tr><tr><td>8</td><td>hat</td><td><span class=\"IdWeight\">IdWeight(0x000015fa, 0.43809587f0)</span>.id</td><td>0.438</td></tr><tr><td>9</td><td>bat</td><td><span class=\"IdWeight\">IdWeight(0x0000133e, 0.43897963f0)</span>.id</td><td>0.439</td></tr><tr><td>10</td><td>fielder</td><td><span class=\"IdWeight\">IdWeight(0x00002801, 0.44676453f0)</span>.id</td><td>0.447</td></tr><tr><td>11</td><td>pitch</td><td><span class=\"IdWeight\">IdWeight(0x00000c1b, 0.45033383f0)</span>.id</td><td>0.45</td></tr><tr><td>12</td><td>helmet</td><td><span class=\"IdWeight\">IdWeight(0x00002b13, 0.4635557f0)</span>.id</td><td>0.464</td></tr><tr><td>13</td><td>catcher</td><td><span class=\"IdWeight\">IdWeight(0x00002369, 0.46529937f0)</span>.id</td><td>0.465</td></tr><tr><td>14</td><td>throw</td><td><span class=\"IdWeight\">IdWeight(0x00000d30, 0.46637392f0)</span>.id</td><td>0.466</td></tr><tr><td>15</td><td>nose</td><td><span class=\"IdWeight\">IdWeight(0x00001737, 0.46784467f0)</span>.id</td><td>0.468</td></tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "search_and_display(index, vocab, vocab2id[\"glove\"], res, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86e60967-b159-4e46-a0a1-369efbe70477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analogias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63a0d1ea-bdf8-468a-a20d-1bdf0c7a6503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000100 seconds (3 allocations: 1.625 KiB)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>resultados for \"&lt;father> - &lt;man> + &lt;woman>\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>mother</td><td><span class=\"IdWeight\">IdWeight(0x00000329, 0.097538054f0)</span>.id</td><td>0.098</td></tr><tr><td>2</td><td>daughter</td><td><span class=\"IdWeight\">IdWeight(0x0000046c, 0.13246548f0)</span>.id</td><td>0.132</td></tr><tr><td>3</td><td>wife</td><td><span class=\"IdWeight\">IdWeight(0x000002bf, 0.14651567f0)</span>.id</td><td>0.147</td></tr><tr><td>4</td><td>father</td><td><span class=\"IdWeight\">IdWeight(0x00000276, 0.14774317f0)</span>.id</td><td>0.148</td></tr><tr><td>5</td><td>husband</td><td><span class=\"IdWeight\">IdWeight(0x00000530, 0.17211103f0)</span>.id</td><td>0.172</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<h2>resultados for \"&lt;father> - &lt;man> + &lt;woman>\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>mother</td><td><span class=\"IdWeight\">IdWeight(0x00000329, 0.097538054f0)</span>.id</td><td>0.098</td></tr><tr><td>2</td><td>daughter</td><td><span class=\"IdWeight\">IdWeight(0x0000046c, 0.13246548f0)</span>.id</td><td>0.132</td></tr><tr><td>3</td><td>wife</td><td><span class=\"IdWeight\">IdWeight(0x000002bf, 0.14651567f0)</span>.id</td><td>0.147</td></tr><tr><td>4</td><td>father</td><td><span class=\"IdWeight\">IdWeight(0x00000276, 0.14774317f0)</span>.id</td><td>0.148</td></tr><tr><td>5</td><td>husband</td><td><span class=\"IdWeight\">IdWeight(0x00000530, 0.17211103f0)</span>.id</td><td>0.172</td></tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000184 seconds (2 allocations: 32 bytes)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>resultados for \"&lt;fireman> - &lt;man> + &lt;woman>\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>fireman</td><td><span class=\"IdWeight\">IdWeight(0x00006ad1, 0.15624285f0)</span>.id</td><td>0.156</td></tr><tr><td>2</td><td>firefighter</td><td><span class=\"IdWeight\">IdWeight(0x00003dc4, 0.30260533f0)</span>.id</td><td>0.303</td></tr><tr><td>3</td><td>paramedic</td><td><span class=\"IdWeight\">IdWeight(0x00008431, 0.39318788f0)</span>.id</td><td>0.393</td></tr><tr><td>4</td><td>rescuer</td><td><span class=\"IdWeight\">IdWeight(0x0000af73, 0.4389633f0)</span>.id</td><td>0.439</td></tr><tr><td>5</td><td>passerby</td><td><span class=\"IdWeight\">IdWeight(0x0000d210, 0.45855498f0)</span>.id</td><td>0.459</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<h2>resultados for \"&lt;fireman> - &lt;man> + &lt;woman>\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>fireman</td><td><span class=\"IdWeight\">IdWeight(0x00006ad1, 0.15624285f0)</span>.id</td><td>0.156</td></tr><tr><td>2</td><td>firefighter</td><td><span class=\"IdWeight\">IdWeight(0x00003dc4, 0.30260533f0)</span>.id</td><td>0.303</td></tr><tr><td>3</td><td>paramedic</td><td><span class=\"IdWeight\">IdWeight(0x00008431, 0.39318788f0)</span>.id</td><td>0.393</td></tr><tr><td>4</td><td>rescuer</td><td><span class=\"IdWeight\">IdWeight(0x0000af73, 0.4389633f0)</span>.id</td><td>0.439</td></tr><tr><td>5</td><td>passerby</td><td><span class=\"IdWeight\">IdWeight(0x0000d210, 0.45855498f0)</span>.id</td><td>0.459</td></tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000090 seconds (2 allocations: 32 bytes)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>resultados for \"&lt;policeman> - &lt;man> + &lt;woman>\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>policeman</td><td><span class=\"IdWeight\">IdWeight(0x00001ac9, 0.14374709f0)</span>.id</td><td>0.144</td></tr><tr><td>2</td><td>wounding</td><td><span class=\"IdWeight\">IdWeight(0x000017e6, 0.2848916f0)</span>.id</td><td>0.285</td></tr><tr><td>3</td><td>policemen</td><td><span class=\"IdWeight\">IdWeight(0x00001378, 0.29457688f0)</span>.id</td><td>0.295</td></tr><tr><td>4</td><td>passerby</td><td><span class=\"IdWeight\">IdWeight(0x0000d210, 0.3055392f0)</span>.id</td><td>0.306</td></tr><tr><td>5</td><td>wounded</td><td><span class=\"IdWeight\">IdWeight(0x00000570, 0.33116198f0)</span>.id</td><td>0.331</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<h2>resultados for \"&lt;policeman> - &lt;man> + &lt;woman>\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>policeman</td><td><span class=\"IdWeight\">IdWeight(0x00001ac9, 0.14374709f0)</span>.id</td><td>0.144</td></tr><tr><td>2</td><td>wounding</td><td><span class=\"IdWeight\">IdWeight(0x000017e6, 0.2848916f0)</span>.id</td><td>0.285</td></tr><tr><td>3</td><td>policemen</td><td><span class=\"IdWeight\">IdWeight(0x00001378, 0.29457688f0)</span>.id</td><td>0.295</td></tr><tr><td>4</td><td>passerby</td><td><span class=\"IdWeight\">IdWeight(0x0000d210, 0.3055392f0)</span>.id</td><td>0.306</td></tr><tr><td>5</td><td>wounded</td><td><span class=\"IdWeight\">IdWeight(0x00000570, 0.33116198f0)</span>.id</td><td>0.331</td></tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000541 seconds (3 allocations: 5.906 KiB)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<h2>resultados for \"&lt;mississippi> - &lt;usa> + &lt;france>\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>louisiana</td><td><span class=\"IdWeight\">IdWeight(0x00000f04, 0.4302075f0)</span>.id</td><td>0.43</td></tr><tr><td>2</td><td>france</td><td><span class=\"IdWeight\">IdWeight(0x00000184, 0.46778142f0)</span>.id</td><td>0.468</td></tr><tr><td>3</td><td>rhine</td><td><span class=\"IdWeight\">IdWeight(0x00003685, 0.48834217f0)</span>.id</td><td>0.488</td></tr><tr><td>4</td><td>coast</td><td><span class=\"IdWeight\">IdWeight(0x000003bb, 0.50627756f0)</span>.id</td><td>0.506</td></tr><tr><td>5</td><td>brittany</td><td><span class=\"IdWeight\">IdWeight(0x00003e05, 0.50635386f0)</span>.id</td><td>0.506</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<h2>resultados for \"&lt;mississippi> - &lt;usa> + &lt;france>\"</h2>\n",
       "<table>\n",
       "<th>  <td>word</td> <td>id</td> <td>dist</td> </th>\n",
       "    <tr><td>1</td><td>louisiana</td><td><span class=\"IdWeight\">IdWeight(0x00000f04, 0.4302075f0)</span>.id</td><td>0.43</td></tr><tr><td>2</td><td>france</td><td><span class=\"IdWeight\">IdWeight(0x00000184, 0.46778142f0)</span>.id</td><td>0.468</td></tr><tr><td>3</td><td>rhine</td><td><span class=\"IdWeight\">IdWeight(0x00003685, 0.48834217f0)</span>.id</td><td>0.488</td></tr><tr><td>4</td><td>coast</td><td><span class=\"IdWeight\">IdWeight(0x000003bb, 0.50627756f0)</span>.id</td><td>0.506</td></tr><tr><td>5</td><td>brittany</td><td><span class=\"IdWeight\">IdWeight(0x00003e05, 0.50635386f0)</span>.id</td><td>0.506</td></tr>\n",
       "</table>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function analogy(a, b, c, k)\n",
    "\tv = index[vocab2id[a]] - index[vocab2id[b]] + index[vocab2id[c]]\n",
    "\tnormalize!(v)\n",
    "\tsearch_and_display(index, vocab, v, res, k, \"<$a> - <$b> + <$c>\")\n",
    "end\n",
    "\n",
    "analogy(\"father\", \"man\", \"woman\", 5)\n",
    "analogy(\"fireman\", \"man\", \"woman\", 5)\n",
    "analogy(\"policeman\", \"man\", \"woman\", 5)\n",
    "analogy(\"mississippi\", \"usa\", \"france\", 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8f09a9-75aa-49c5-9e35-4c486defa289",
   "metadata": {},
   "source": [
    "# Actividades \n",
    "- Como se menciono, el uso de redes neuronales se vuelve muy popular para aprender la semántica desde el texto. Comente las razones justificando sus comentarios en términos de aprendizaje y requerimientos computacionales.\n",
    "- Discuta que tipo de métrica se usa y porque.\n",
    "- ¿Cómo se podrían usar los word-embeddings para búsqueda de documentos completos?\n",
    "- Como ajustaría la métrica.\n",
    "- Si usa Julia, revisé el paquete `Embeddings.jl`, `Word2Vec.jl`.\n",
    "- Si usa Python, revisé los paquetes `fastText` y `word2vec`.\n",
    "- Glove <https://nlp.stanford.edu/projects/glove/>\n",
    "- Reproduzca el ejercicio de este notebook (no reproduzca la búsqueda exhaustiva, ya que le tomará muchísimo tiempo), use embeddings para español, cambié los ejemplos. Se sugiere el uso de <https://ingeotec.github.io/regional-spanish-models/> donde encontrará modelos fastText regionalizados del español, pero puede usar otros embeddings.\n",
    "- Reporte su notebook y anote sus soluciones a las preguntas planteadas. Anote sus reflexiones.\n",
    "\n",
    "# Bibliografía\n",
    "- [Dumais2004] Dumais, S. T. (2004). Latent semantic analysis. Annu. Rev. Inf. Sci. Technol., 38(1), 188-230.\n",
    "- [Hofmann1999] Hofmann, T. (1999, August). Probabilistic latent semantic indexing. In Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval (pp. 50-57).\n",
    "- [MCCD2013]: Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781.\n",
    "- [PSM2014] Pennington, J., Socher, R., & Manning, C. D. (2014, October). Glove: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543).\n",
    "- [BGJM2017]: Bojanowski, P., Grave, E., Joulin, A., & Mikolov, T. (2017). Enriching word vectors with subword information. Transactions of the association for computational linguistics, 5, 135-146.\n",
    "- [VSPU2017] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0-rc1",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
